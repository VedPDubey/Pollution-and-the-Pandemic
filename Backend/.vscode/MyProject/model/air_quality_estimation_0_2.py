# -*- coding: utf-8 -*-
"""Air_Quality_Estimation 0.2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MZwfh2grBn_4sycAtOCPetVQWqGmlc1g

**Importing Necessary Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#import folium 
import plotly.express as px
import datetime
from math import pi

import missingno as msno
from sklearn.impute import KNNImputer
# Autoreg, autocorrolationand time series tools...

from pandas.plotting import lag_plot, autocorrelation_plot
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.metrics import mean_squared_error
#from statsmodels.tsa.ar_model import AutoReg
#from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic

from fbprophet import Prophet
from fbprophet.diagnostics import mape,cross_validation,performance_metrics
from fbprophet.plot import plot_plotly, plot_components_plotly

plt.style.use('seaborn-whitegrid')

from termcolor import colored

import pickle

#from google.colab import files
#uploaded = files.upload()

"""**Exploratory Data Analysis**"""

city_day = pd.read_csv('C:\Users\Ved Prakash Dubey\Documents\Hackathon\.vscode\MyProject\model\city_day.csv').sort_values(by = ['Date', 'City'])

print(list(city_day.columns))

city_day.head()

city_day.info()

city_day.Date = city_day.Date.apply(lambda x : datetime.datetime.strptime(x, '%Y-%m-%d'))
city_day = city_day.sort_values(by = 'Date')

print('Date starts from {}, and ends in {}'.format(city_day.Date.min().strftime('%Y-%m-%d'), city_day.Date.max().strftime('%Y-%m-%d')))

city_day.corr().AQI.sort_values(ascending = False)

# adding all the features with corr less than 0.4

city_day['B_X_O3_NH3'] = city_day['Benzene'] +\
city_day['Xylene'] + city_day['O3'] + city_day['NH3']

city_day['ParticulateMatters'] = city_day['PM2.5'] + city_day['PM10']

corr_with_AQI = city_day.corr().AQI.sort_values(ascending = False)


corr_with_AQI
# from here we can see: we can impute values with linear
# interpolation for the ones that have high value of corr

# how much is the average amount of pollution in each city stations
most_polluted = city_day[['City', 'AQI', 'PM10', 'CO']].groupby(['City']).mean().sort_values(by = 'AQI', ascending = False)
most_polluted

most_polluted = city_day[['City', 'AQI', 'PM10', 'CO']].groupby(['City']).mean().sort_values(by = 'AQI', ascending = False)

cities = most_polluted.index
params = most_polluted.columns

def first_date(city, parameter):
    df = city_day[(city_day.City == city)]
    df = df[df[parameter].notnull()]
    if len(df) != 0:
        return df.iloc[0].Date.strftime('%Y-%m-%d')
    else: return('no_measurement')
        
        
for city in cities:
    #print(colored('city: ', 'green'), city)
    for param in params:
      #  print('param: ', param)
        most_polluted.loc[city, str(param) + '_date'] = first_date(city, param)
        
most_polluted.head()

city_day['Year_Month'] = city_day.Date.apply(lambda x : x.strftime('%Y-%m'))

df = city_day.groupby(['Year_Month']).sum().reset_index()

# let's only see those that are important to the AQI
# otherwise we will have a messy plot

metrices = corr_with_AQI[corr_with_AQI>0.5].index

cities = ['Ahmedabad','Delhi','Bengaluru','Mumbai','Hyderabad','Chennai']

filtered_city_day = city_day[city_day['Date'] >= '2019-01-01']
AQI = filtered_city_day[filtered_city_day.City.isin(cities)][['Date','City','AQI','AQI_Bucket']]
AQI.head()

"""**Forecasting**"""

def tell_me_null(df):
    num_null = df.isnull().sum().sort_values(ascending = False)
    percentage_null = round(df.isnull().sum().sort_values(ascending = False)/len(df) * 100, 1)
    return pd.DataFrame(np.c_[num_null, percentage_null], index = num_null.index,  columns = ['# of Null', 'Percentage'])

tell_me_null(city_day)

def predict(givencity):
  givencity = city_day[(city_day.AQI.notnull()) & (city_day.City == givencity)]
  #tell_me_null(givencity)

  corr = givencity.corr().AQI.sort_values(ascending = False)
  related = list(corr[corr>0.6].index)
  #print(related)

  inter = givencity.loc[:, related].interpolate(method = 'linear')
  givencity.loc[:, related] = inter
  knn_imputer = KNNImputer(n_neighbors = 3)

  imputing_cols = [ 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2',
        'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'B_X_O3_NH3',
        'ParticulateMatters']
  # we eliminated city, date, Year_Month and AQI_Bucket because 
  # they either were unique or had numerical substitutes in other fields(AQI_bucket)

  knn_imputer.fit(givencity[imputing_cols])

  imputed = knn_imputer.transform(givencity[imputing_cols])

  #givencity.loc[:, imputing_cols] = imputed

  #tell_me_null(givencity)

  givencity_aqi = givencity[['Date','AQI']]
  givencity_aqi.reset_index(inplace = True,drop = True)

  train_df = givencity_aqi
  train_df.rename(mapper = {'Date':'ds','AQI':'y'},axis =1,inplace = True)
  train_df

  m = Prophet(holidays_prior_scale=0,seasonality_prior_scale=20,n_changepoints= 50)

  m.fit(train_df)
  future = m.make_future_dataframe(periods=365)
  #future.tail()
  forecast = m.predict(future)
  forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

  #df_cv = cross_validation(m, initial='1100 days', period='121 days', horizon = '365 days')
  #df_p = performance_metrics(df_cv)
  #print('Cross Validation accuracy:', (1 - df_p['mape'].mean())*100)

  predictions_df=pd.DataFrame(forecast,columns=['ds','yhat'])

  return predictions_df,m

kolkata,mk = predict('Kolkata')

chennai,mCh = predict('Chennai')

hyderabad,mH = predict('Hyderabad')

mumbai,mMu = predict('Mumbai')

delhi,mD = predict('Delhi')

with open("modelkolkata", 'wb') as file:  
    pickle.dump(mk, file)

with open("modelchennai", 'wb') as file:  
    pickle.dump(mCh, file)

with open("modeldelhi", 'wb') as file:  
    pickle.dump(mD, file)

with open("modelhyderabad", 'wb') as file:  
    pickle.dump(mH, file)

with open("modelmumbai", 'wb') as file:  
    pickle.dump(mMu, file)